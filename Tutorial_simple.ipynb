{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing our own models\n",
    "import helper\n",
    "from analyzer import Analyzer\n",
    "from crawler import Crawler\n",
    "from plotter import Plotter\n",
    "from model import TextBlob, TrainedSentimentModel, Vader\n",
    "\n",
    "# Additional imports\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\t\"user_auth\": False, # autheticate as user or application\n",
    "\t\"auth_path\": \"/home/maxi/Documents/UNI/Ethics/Project/repo/Corona_Sentinent/\", # path to auth\n",
    "    \"search\": {\n",
    "        \"location\": \"darmstadt\", # based on helper.GEOCODES dictionary\n",
    "        \"radius\": 100, # optional default to 100\n",
    "        \"query\": ['die', 'der', 'das'], # query for searching (str array), either query or location has to be not empty\n",
    "        \"max_searches\": 1000, # Default: 1000 max amount of searches \n",
    "        \"num_results\": 10, # number of results with defined filter options\n",
    "        \"rate_limit\": False,  # Default True: to turn off rate limit prints\n",
    "        \"filter\": { # Filter applies to search\n",
    "            \"not_reply\": True, # Filter removes replies when true, does nothing when false\n",
    "            \"not_retweet\": True, # Filter removes retweets when true, does nothing when false\n",
    "            \"until\": datetime.datetime(2020, 3, 1), # None or datetime (e.g. datetime.datetime(2020, 5, 20))\n",
    "                                                    # to filter tweets until specified time.\n",
    "        }\n",
    "    },\n",
    "    \"analyze_sentiment\": {\n",
    "#         \"pos_boundary\": 0.5, # boundary for classifying tweets as \"extremely\" positive\n",
    "#         \"neg_boundary\": 0.5, # boundary for classifying tweets as \"extremely\" negative\n",
    "        \"users_dir\": \"saved_data/full_scan_both/results/all/\"  # to store sentiment analysis files for future\n",
    "    },\n",
    "\t\"get_user\": { # Optional, mandatory only when querying for users\n",
    "\t\t\"good_user\": True, # mandatory: Filtering users with too many / too few tweets\n",
    "\t\t\"search_type\": \"recent_user\", # 'recent_user', 'recent_retweeted_user'\n",
    "\t\t\"num_users\": 40, # mandatory: How many users to return in search\n",
    "        \"unique_ids\": True, # If True will remember user ids in session for several scans\n",
    "\t},\n",
    "\t\"plot\": {\n",
    "\t\t\"title\": \"Testing\",\n",
    "\t},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates full config file, important to run everytime something inside the config has changed!\n",
    "config = helper.init_config(config)\n",
    "\n",
    "# Creating the crawler and the plotter objects based on the configs\n",
    "crawler = Crawler(config)\n",
    "plotter = Plotter(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The first and easiest way to get to data is to use this call. Based on your configurations it will \n",
    "give you different tweets. By changing the config file and realoading the crawler (previous cell), you can\n",
    "quickly change the results \"\"\"\n",
    "tweets = crawler.get_tweets()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "<class 'tweepy.models.Status'>\n",
      "Ende November soll in #Kefenrod ein Flurbereinigungsverfahren eingeleitet werden. Der Kreis-Anzeiger erklärt, was d… https://t.co/NmO4vJ9meQ\n",
      "1320395975123476480\n"
     ]
    }
   ],
   "source": [
    "\"\"\" tweets is now a list of tweepy Status objects \"\"\"\n",
    "print(len(tweets))\n",
    "print(type(tweets[0]))\n",
    "\n",
    "\"\"\" Using the properties of this object one can get information about the tweet, e.g. \"\"\"\n",
    "print(tweets[0].text)\n",
    "print(tweets[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" For a more advanced scraping of data, we crawl random users which have\n",
    "tweeted recently and add those user ids to our list. This step can take some time, \n",
    "dependent on the \"\"\"\n",
    "users = crawler.get_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" With the list of those users, we can look at the timeline of each user.\"\"\"\n",
    "timeline = crawler.get_timeline(users[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Timeline is a list of tweets only from this user \"\"\"\n",
    "print(\"Number of tweets on timeline: \", len(timeline))\n",
    "print(type(timeline[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to analyze our scraped data we load different models to run the analyzation on.\n",
    "We chose 2 pre trained models: TextBlob and Vader. TextBlob is the most\n",
    "simple model, Vader a little bit more advanced and then our model, loaded\n",
    "here with TrainedSentimentModel, which is optimized on German Tweets.\n",
    "\"\"\"\n",
    "\n",
    "# Load 3 different models\n",
    "blob_model = TextBlob()\n",
    "vader_model = Vader()\n",
    "our_model = TrainedSentimentModel()\n",
    "\n",
    "# Configure which model should be used for analyzation\n",
    "analyzer_blob = Analyzer(config, blob_model)\n",
    "analyzer_vader = Analyzer(config, blob_model)\n",
    "analyzer_our = Analyzer(config, blob_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" With this simple script we analyze the timeline of all users and plot those on \n",
    "a simple histogram to analyze the overall sentiment \"\"\"\n",
    "sentis = analyzer_blob.analyze_sentiment(timeline)\n",
    "plotter.simple_hist(sentis)\n",
    "\n",
    "sentis = analyzer_vader.analyze_sentiment(timeline)\n",
    "plotter.simple_hist(sentis)\n",
    "\n",
    "sentis = analyzer_our.analyze_sentiment(timeline)\n",
    "plotter.simple_hist(sentis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ethics",
   "language": "python",
   "name": "ethics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
